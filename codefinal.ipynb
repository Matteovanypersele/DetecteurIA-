{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1954121-18af-4845-8f8e-8c63c6d68ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/site-packages (25.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.1\n",
      "    Uninstalling pip-25.1:\n",
      "      Successfully uninstalled pip-25.1\n",
      "Successfully installed pip-25.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ab90e9-3c9a-4e28-86aa-283bb42a4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dirtyjson\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: dirtyjson\n",
      "Successfully installed dirtyjson-1.0.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dirtyjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c480618e-c898-460e-87b0-a0c3f32cdd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch) (80.0.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2.post1\u001b[0m \u001b[32m 3/20\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2.post1:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/20\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2.post1━━━━━━\u001b[0m \u001b[32m 4/20\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [torchaudio]0\u001b[0m [torchaudio]]ver-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.13.1 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface-hub-0.30.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers scikit-learn pandas tqdm joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb84743-fa82-44ae-ae3b-71485683bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dirtyjson as dj  # pip install dirtyjson\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8380d63-8c8b-4605-9cbf-377a92dc895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ valid-dirty.jsonl         → valid-dirty_enriched.jsonl — 737 objets enrichis\n",
      "✓ train-dirty.jsonl         → train-dirty_enriched.jsonl — 2506 objets enrichis\n",
      "✓ test-dirty.jsonl          → test-dirty_enriched.jsonl — 1762 objets enrichis\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Enrichit les JSONL OpenLLMText_Human avec des métriques clavier authentic\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Prérequis  : pip install pandas numpy dirtyjson\n",
    "Exécution  : python enrich_openllmtext.py\n",
    "Structure  :\n",
    "    .\n",
    "    ├── ai_detect_keystroke_logging_data_anon_github.csv\n",
    "    └── OpenLLMText_Human/\n",
    "        ├── train-dirty.jsonl\n",
    "        ├── valid-dirty.jsonl\n",
    "        └── test-dirty.jsonl\n",
    "Le script crée pour chaque fichier *_dirty.jsonl* un *_enriched.jsonl*.\n",
    "\"\"\"\n",
    "\n",
    "import os, json, random, numpy as np, pandas as pd, dirtyjson as dj\n",
    "from pathlib import Path\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 1. PARAMÈTRES\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "CSV_PATH = \"ai_detect_keystroke_logging_data_anon_github.csv\"\n",
    "DATASET_DIRS = [\"OpenLLMText_Human\"]      # ajoutez d’autres dossiers si besoin\n",
    "OUT_SUFFIX = \"_enriched.jsonl\"            # suffixe de sortie\n",
    "RANDOM_SEED = 42                          # pour reproductibilité du sample\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 2. COLONNES À EXTRAIRE (20 demandées + condition)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "COLS = [\n",
    "    \"mean_pause_time_in_secs_threshold200\",\n",
    "    \"total_insertions_chars_exclu_space\",\n",
    "    \"total_pause_time_in_secs_threshold2000\",\n",
    "    \"product_process_ratio\",\n",
    "    \"mean_insertion_length_chars_exclu_space\",\n",
    "    \"total_deletions_words\",\n",
    "    \"mean_pause_time_before_sents_threshold200\",\n",
    "    \"mean_deletion_length_chars\",\n",
    "    \"num_of_insertions\",\n",
    "    \"median_insertion_length_chars_exclu_space\",\n",
    "    \"num_of_pause_within_words_threshold200\",\n",
    "    \"sd_strokes_per_min_5_intervals\",\n",
    "    \"median_length_Rburst_sec\",\n",
    "    \"median_pause_time_between_words_threshold200\",\n",
    "    \"num_of_pause_after_words_threshold200\",\n",
    "    \"num_of_revisions\",\n",
    "    \"sd_pause_time_before_words_threshold200\",\n",
    "    \"total_number_of_pauses_threshold2000\",\n",
    "    \"median_pause_time_before_words_threshold200\",\n",
    "    \"mean_pause_time_before_words_threshold200\",\n",
    "]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 3. CHARGER LE CSV & CONTRÔLER LES COLONNES\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "missing = set(COLS + [\"condition\"]) - set(pd.read_csv(CSV_PATH, nrows=0).columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes dans le CSV: {missing}\")\n",
    "\n",
    "df_auth = (\n",
    "    pd.read_csv(CSV_PATH, usecols=[\"condition\"] + COLS)\n",
    "      .query(\"condition == 'authentic'\")\n",
    "      .reset_index(drop=True)[COLS]\n",
    ")\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 4. UTILITAIRE : LIRE UN OBJET JSON, MÊME SUR PLUSIEURS LIGNES\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def iter_json_objects(path):\n",
    "    \"\"\"Yield chaque objet JSON contenu dans *path*, même s’il s’étale sur\n",
    "       plusieurs lignes. Utilise un comptage d'accolades et dirtyjson.\"\"\"\n",
    "    buf, depth = [], 0\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            depth += line.count(\"{\") - line.count(\"}\")\n",
    "            buf.append(line)\n",
    "            if depth == 0 and buf:\n",
    "                yield dj.loads(\"\".join(buf))\n",
    "                buf.clear()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 5. FONCTION D’ENRICHISSEMENT\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def enrich_jsonl(file_path: Path, text_source: str) -> None:\n",
    "    \"\"\"Crée <file_path>_enriched.jsonl avec un champ 'extra' additionnel.\"\"\"\n",
    "    out_path = file_path.with_name(file_path.stem + OUT_SUFFIX)\n",
    "    n = 0\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for item in iter_json_objects(file_path):\n",
    "            #  échantillonner UNE ligne authentic aléatoire\n",
    "            meta_row = df_auth.iloc[rng.integers(len(df_auth))].to_dict()\n",
    "            # convertir les types numpy -> natifs\n",
    "            meta_row = {k: (None if pd.isna(v)\n",
    "                            else v.item() if hasattr(v, \"item\") else v)\n",
    "                        for k, v in meta_row.items()}\n",
    "            # fusionner dans item[\"extra\"]\n",
    "            item.setdefault(\"extra\", {}).update(\n",
    "                {\"text_source\": text_source, **meta_row}\n",
    "            )\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            n += 1\n",
    "    print(f\"✓ {file_path.name:25s} → {out_path.name:25s} — {n} objets enrichis\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. BOUCLE PRINCIPALE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    for folder in DATASET_DIRS:\n",
    "        for fname in os.listdir(folder):\n",
    "            if fname.endswith(\".jsonl\") and \"_enriched\" not in fname:\n",
    "\n",
    "                fpath = Path(folder) / fname\n",
    "                enrich_jsonl(\n",
    "                    fpath,\n",
    "                    text_source=folder.replace(\"OpenLLMText_\", \"\").rstrip(\"/\"),\n",
    "                )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "391cd50b-6252-40f0-8a0b-f40c4936d277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ valid-dirty.jsonl         → valid-dirty_enriched.jsonl — 11708 objets enrichis\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Invalid \\X escape sequence 'o': line 1 column 391 (char 390)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/dirtyjson/loader.py:250\u001b[39m, in \u001b[36mDirtyJSONLoader.parse_string\u001b[39m\u001b[34m(self, terminating_character, _b, _join, _py2, _maxunicode)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     char = \u001b[43m_b\u001b[49m\u001b[43m[\u001b[49m\u001b[43mesc\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'o'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    117\u001b[39m                 enrich_jsonl(\n\u001b[32m    118\u001b[39m                     fpath,\n\u001b[32m    119\u001b[39m                     text_source=folder.replace(\u001b[33m\"\u001b[39m\u001b[33mOpenLLMText_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).rstrip(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    120\u001b[39m                 )\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fname.endswith(\u001b[33m\"\u001b[39m\u001b[33m.jsonl\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_enriched\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m fname:\n\u001b[32m    116\u001b[39m     fpath = Path(folder) / fname\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[43menrich_jsonl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_source\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOpenLLMText_\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36menrich_jsonl\u001b[39m\u001b[34m(file_path, text_source)\u001b[39m\n\u001b[32m     91\u001b[39m n = \u001b[32m0\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m out_path.open(\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fout:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_json_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  échantillonner UNE ligne authentic aléatoire\u001b[39;49;00m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeta_row\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_auth\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_auth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# convertir les types numpy -> natifs\u001b[39;49;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36miter_json_objects\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     80\u001b[39m buf.append(line)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m depth == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m buf:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mdj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     buf.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/dirtyjson/__init__.py:94\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, encoding, parse_float, parse_int, parse_constant, search_for_first_object, start_index)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Deserialize ``s`` (a ``str`` or ``unicode`` instance containing a JSON\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03mdocument) to a Python object.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03mencountered.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m d = DirtyJSONLoader(s, encoding, parse_float, parse_int, parse_constant)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_for_first_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/dirtyjson/loader.py:372\u001b[39m, in \u001b[36mDirtyJSONLoader.decode\u001b[39m\u001b[34m(self, search_for_first_object, start_index)\u001b[39m\n\u001b[32m    369\u001b[39m         \u001b[38;5;28mself\u001b[39m._skip_forward_to(i)\n\u001b[32m    371\u001b[39m \u001b[38;5;28mself\u001b[39m._skip_whitespace()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/dirtyjson/loader.py:173\u001b[39m, in \u001b[36mDirtyJSONLoader.scan\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parse_string(nextchar)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nextchar == \u001b[33m'\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nextchar == \u001b[33m'\u001b[39m\u001b[33m[\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parse_array()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/dirtyjson/loader.py:318\u001b[39m, in \u001b[36mDirtyJSONLoader.parse_object\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28mself\u001b[39m._skip_whitespace()\n\u001b[32m    317\u001b[39m key_value_pos = KeyValuePosition(key_pos, \u001b[38;5;28mself\u001b[39m._current_position())\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m obj.add_with_attributes(key, value, key_value_pos)\n\u001b[32m    321\u001b[39m nextchar = \u001b[38;5;28mself\u001b[39m._next_character_after_whitespace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/dirtyjson/loader.py:171\u001b[39m, in \u001b[36mDirtyJSONLoader.scan\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    168\u001b[39m nextchar = \u001b[38;5;28mself\u001b[39m._next_character()\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nextchar == \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m nextchar == \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnextchar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nextchar == \u001b[33m'\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parse_object()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/dirtyjson/loader.py:253\u001b[39m, in \u001b[36mDirtyJSONLoader.parse_string\u001b[39m\u001b[34m(self, terminating_character, _b, _join, _py2, _maxunicode)\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    252\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mInvalid \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mX escape sequence \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Error(msg, \u001b[38;5;28mself\u001b[39m.content, \u001b[38;5;28mself\u001b[39m.pos)\n\u001b[32m    254\u001b[39m     \u001b[38;5;28mself\u001b[39m.pos += \u001b[32m1\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    256\u001b[39m     \u001b[38;5;66;03m# Unicode escape sequence\u001b[39;00m\n",
      "\u001b[31mError\u001b[39m: Invalid \\X escape sequence 'o': line 1 column 391 (char 390)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Enrichit les JSONL OpenLLMText_Human avec des métriques clavier authentic\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Prérequis  : pip install pandas numpy dirtyjson\n",
    "Exécution  : python enrich_openllmtext.py\n",
    "Structure  :\n",
    "    .\n",
    "    ├── ai_detect_keystroke_logging_data_anon_github.csv\n",
    "    └── OpenLLMText_Human/\n",
    "        ├── train-dirty.jsonl\n",
    "        ├── valid-dirty.jsonl\n",
    "        └── test-dirty.jsonl\n",
    "Le script crée pour chaque fichier *_dirty.jsonl* un *_enriched.jsonl*.\n",
    "\"\"\"\n",
    "\n",
    "import os, json, random, numpy as np, pandas as pd, dirtyjson as dj\n",
    "from pathlib import Path\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 1. PARAMÈTRES\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "CSV_PATH = \"ai_detect_keystroke_logging_data_anon_github.csv\"\n",
    "DATASET_DIRS = [\"OpenLLMText_ChatGPT\"]      # ajoutez d’autres dossiers si besoin\n",
    "OUT_SUFFIX = \"_enriched.jsonl\"            # suffixe de sortie\n",
    "RANDOM_SEED = 42                          # pour reproductibilité du sample\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 2. COLONNES À EXTRAIRE (20 demandées + condition)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "COLS = [\n",
    "    \"mean_pause_time_in_secs_threshold200\",\n",
    "    \"total_insertions_chars_exclu_space\",\n",
    "    \"total_pause_time_in_secs_threshold2000\",\n",
    "    \"product_process_ratio\",\n",
    "    \"mean_insertion_length_chars_exclu_space\",\n",
    "    \"total_deletions_words\",\n",
    "    \"mean_pause_time_before_sents_threshold200\",\n",
    "    \"mean_deletion_length_chars\",\n",
    "    \"num_of_insertions\",\n",
    "    \"median_insertion_length_chars_exclu_space\",\n",
    "    \"num_of_pause_within_words_threshold200\",\n",
    "    \"sd_strokes_per_min_5_intervals\",\n",
    "    \"median_length_Rburst_sec\",\n",
    "    \"median_pause_time_between_words_threshold200\",\n",
    "    \"num_of_pause_after_words_threshold200\",\n",
    "    \"num_of_revisions\",\n",
    "    \"sd_pause_time_before_words_threshold200\",\n",
    "    \"total_number_of_pauses_threshold2000\",\n",
    "    \"median_pause_time_before_words_threshold200\",\n",
    "    \"mean_pause_time_before_words_threshold200\",\n",
    "]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 3. CHARGER LE CSV & CONTRÔLER LES COLONNES\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "missing = set(COLS + [\"condition\"]) - set(pd.read_csv(CSV_PATH, nrows=0).columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes dans le CSV: {missing}\")\n",
    "\n",
    "df_auth = (\n",
    "    pd.read_csv(CSV_PATH, usecols=[\"condition\"] + COLS)\n",
    "      .query(\"condition == 'transcribed'\")\n",
    "      .reset_index(drop=True)[COLS]\n",
    ")\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 4. UTILITAIRE : LIRE UN OBJET JSON, MÊME SUR PLUSIEURS LIGNES\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def iter_json_objects(path):\n",
    "    \"\"\"Yield chaque objet JSON contenu dans *path*, même s’il s’étale sur\n",
    "       plusieurs lignes. Utilise un comptage d'accolades et dirtyjson.\"\"\"\n",
    "    buf, depth = [], 0\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            depth += line.count(\"{\") - line.count(\"}\")\n",
    "            buf.append(line)\n",
    "            if depth == 0 and buf:\n",
    "                yield dj.loads(\"\".join(buf))\n",
    "                buf.clear()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 5. FONCTION D’ENRICHISSEMENT\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def enrich_jsonl(file_path: Path, text_source: str) -> None:\n",
    "    \"\"\"Crée <file_path>_enriched.jsonl avec un champ 'extra' additionnel.\"\"\"\n",
    "    out_path = file_path.with_name(file_path.stem + OUT_SUFFIX)\n",
    "    n = 0\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for item in iter_json_objects(file_path):\n",
    "            #  échantillonner UNE ligne authentic aléatoire\n",
    "            meta_row = df_auth.iloc[rng.integers(len(df_auth))].to_dict()\n",
    "            # convertir les types numpy -> natifs\n",
    "            meta_row = {k: (None if pd.isna(v)\n",
    "                            else v.item() if hasattr(v, \"item\") else v)\n",
    "                        for k, v in meta_row.items()}\n",
    "            # fusionner dans item[\"extra\"]\n",
    "            item.setdefault(\"extra\", {}).update(\n",
    "                {\"text_source\": text_source, **meta_row}\n",
    "            )\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            n += 1\n",
    "    print(f\"✓ {file_path.name:25s} → {out_path.name:25s} — {n} objets enrichis\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. BOUCLE PRINCIPALE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    for folder in DATASET_DIRS:\n",
    "        for fname in os.listdir(folder):\n",
    "            if fname.endswith(\".jsonl\") and \"_enriched\" not in fname:\n",
    "\n",
    "                fpath = Path(folder) / fname\n",
    "                enrich_jsonl(\n",
    "                    fpath,\n",
    "                    text_source=folder.replace(\"OpenLLMText_\", \"\").rstrip(\"/\"),\n",
    "                )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6731535f-7594-4de8-831b-fc2fa5400871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ train-dirty_rejects.jsonl → train-dirty_rejects_enriched.jsonl — 0 enrichis, 0 rejetés\n",
      "✓ valid-dirty.jsonl         → valid-dirty_enriched.jsonl — 11708 enrichis, 0 rejetés\n",
      "✓ train-dirty.jsonl         → train-dirty_enriched.jsonl — 27722 enrichis, 0 rejetés\n",
      "✓ valid-dirty_rejects.jsonl → valid-dirty_rejects_enriched.jsonl — 0 enrichis, 0 rejetés\n",
      "✓ test-dirty.jsonl          → test-dirty_enriched.jsonl — 780 enrichis, 0 rejetés\n",
      "✓ test-dirty_rejects.jsonl  → test-dirty_rejects_enriched.jsonl — 0 enrichis, 0 rejetés\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Enrichit les JSONL OpenLLMText_Human et consigne les objets invalides\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    ai_detect_keystroke_logging_data_anon_github.csv\n",
    "    OpenLLMText_Human/\n",
    "        ├── train-dirty.jsonl\n",
    "        ├── valid-dirty.jsonl\n",
    "        └── test-dirty.jsonl\n",
    "\n",
    "Résultat :\n",
    "    OpenLLMText_Human/\n",
    "        ├── train-dirty_enriched.jsonl\n",
    "        ├── train-dirty_rejects.jsonl\n",
    "        └── …\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, random, numpy as np, pandas as pd, dirtyjson as dj\n",
    "from pathlib import Path\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# 1. PARAMÈTRES\n",
    "# ──────────────────────────────────────────────────\n",
    "CSV_PATH     = \"ai_detect_keystroke_logging_data_anon_github.csv\"\n",
    "DATASET_DIRS = [\"OpenLLMText_ChatGPT\"]\n",
    "OUT_SUFFIX   = \"_enriched.jsonl\"\n",
    "REJ_SUFFIX   = \"_rejects.jsonl\"\n",
    "RANDOM_SEED  = 42\n",
    "\n",
    "# 20 colonnes demandées\n",
    "COLS = [\n",
    "    \"mean_pause_time_in_secs_threshold200\",\n",
    "    \"total_insertions_chars_exclu_space\",\n",
    "    \"total_pause_time_in_secs_threshold2000\",\n",
    "    \"product_process_ratio\",\n",
    "    \"mean_insertion_length_chars_exclu_space\",\n",
    "    \"total_deletions_words\",\n",
    "    \"mean_pause_time_before_sents_threshold200\",\n",
    "    \"mean_deletion_length_chars\",\n",
    "    \"num_of_insertions\",\n",
    "    \"median_insertion_length_chars_exclu_space\",\n",
    "    \"num_of_pause_within_words_threshold200\",\n",
    "    \"sd_strokes_per_min_5_intervals\",\n",
    "    \"median_length_Rburst_sec\",\n",
    "    \"median_pause_time_between_words_threshold200\",\n",
    "    \"num_of_pause_after_words_threshold200\",\n",
    "    \"num_of_revisions\",\n",
    "    \"sd_pause_time_before_words_threshold200\",\n",
    "    \"total_number_of_pauses_threshold2000\",\n",
    "    \"median_pause_time_before_words_threshold200\",\n",
    "    \"mean_pause_time_before_words_threshold200\",\n",
    "]\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# 2. CHARGER LE CSV AUTHENTIC\n",
    "# ──────────────────────────────────────────────────\n",
    "missing = set(COLS + [\"condition\"]) - set(pd.read_csv(CSV_PATH, nrows=0).columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes dans le CSV : {missing}\")\n",
    "\n",
    "df_auth = (\n",
    "    pd.read_csv(CSV_PATH, usecols=[\"condition\"] + COLS)\n",
    "      .query(\"condition == 'transcribed'\")\n",
    "      .reset_index(drop=True)[COLS]\n",
    ")\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# 3. GÉNÉRATEUR ROBUSTE + CONSIGNATION DES OBJETS CASSÉS\n",
    "# ──────────────────────────────────────────────────\n",
    "def iter_json_objects(path: Path, reject_path: Path | None = None):\n",
    "    \"\"\"Yield chaque objet JSON complet, même multi‑lignes.\n",
    "       Les objets indécodables sont écrits dans *reject_path* et ignorés.\"\"\"\n",
    "    buf, depth = [], 0\n",
    "    with path.open(encoding=\"utf-8\") as fin, \\\n",
    "         (reject_path.open(\"w\", encoding=\"utf-8\") if reject_path\n",
    "          else nullcontext()) as rej:\n",
    "        for line in fin:\n",
    "            depth += line.count(\"{\") - line.count(\"}\")\n",
    "            buf.append(line)\n",
    "            if depth == 0 and buf:\n",
    "                raw = \"\".join(buf)\n",
    "                try:\n",
    "                    yield dj.loads(raw)\n",
    "                except Exception as e:\n",
    "                    if rej:\n",
    "                        rej.write(raw + \"\\n\")     # consigne le JSON brut\n",
    "                    else:\n",
    "                        print(f\"⚠︎ Rejeté ({e})\")\n",
    "                buf.clear()\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# 4. ENRICHISSEMENT\n",
    "# ──────────────────────────────────────────────────\n",
    "def enrich_jsonl(file_path: Path, text_source: str) -> None:\n",
    "    out_path = file_path.with_name(file_path.stem + OUT_SUFFIX)\n",
    "    rej_path = file_path.with_name(file_path.stem + REJ_SUFFIX)\n",
    "    n_ok = n_rej = 0\n",
    "\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for item in iter_json_objects(file_path, reject_path=rej_path):\n",
    "            if item is None:            # objet rejeté\n",
    "                n_rej += 1\n",
    "                continue\n",
    "\n",
    "            # 1 ligne authentic aléatoire\n",
    "            meta_row = df_auth.iloc[rng.integers(len(df_auth))].to_dict()\n",
    "            meta_row = {k: (None if pd.isna(v) else v.item()\n",
    "                            if hasattr(v, \"item\") else v)\n",
    "                        for k, v in meta_row.items()}\n",
    "\n",
    "            item.setdefault(\"extra\", {}).update(\n",
    "                {\"text_source\": text_source, **meta_row}\n",
    "            )\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            n_ok += 1\n",
    "\n",
    "    print(f\"✓ {file_path.name:25s} → {out_path.name:25s} — \"\n",
    "          f\"{n_ok} enrichis, {n_rej} rejetés\")\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# 5. BOUCLE PRINCIPALE\n",
    "# ──────────────────────────────────────────────────\n",
    "def main():\n",
    "    for folder in DATASET_DIRS:\n",
    "        for fname in os.listdir(folder):\n",
    "            if fname.endswith(\".jsonl\") and \"_enriched\" not in fname:\n",
    "                fpath = Path(folder) / fname\n",
    "                enrich_jsonl(\n",
    "                    fpath,\n",
    "                    text_source=folder.replace(\"OpenLLMText_\", \"\").rstrip(\"/\"),\n",
    "                )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbd9c3-5320-4460-ab2f-a0721b6ff0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size : 51874 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=0.3162 val_loss=0.2698  F1=0.936  ROC‑AUC=0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=0.2156 val_loss=0.1761  F1=0.977  ROC‑AUC=0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 192\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(losses), np.array(preds), np.array(trues)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     tr_loss, _, _       = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     val_loss, y_hat, y_true = run_epoch(test_loader,  train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    195\u001b[39m     f1  = f1_score(y_true, y_hat >= \u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(loader, train)\u001b[39m\n\u001b[32m    183\u001b[39m     loss.backward()\n\u001b[32m    184\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    187\u001b[39m preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n\u001b[32m    188\u001b[39m trues.extend(y.cpu().numpy())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  Text‑+‑Meta  •  clean split (800 logs train / 200 logs eval)\n",
    "#  Computes F1, ROC‑AUC and false‑positive rate\n",
    "# ================================================================\n",
    "import json, os, random, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "# -------------------- Parameters ---------------------------------\n",
    "SEED       = 42\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 2\n",
    "LR         = 2e-5\n",
    "BACKBONE   = \"roberta-base\"\n",
    "\n",
    "META_COLS = [\n",
    "    \"mean_pause_time_in_secs_threshold200\",\n",
    "    \"total_insertions_chars_exclu_space\",\n",
    "    \"total_pause_time_in_secs_threshold2000\",\n",
    "    \"product_process_ratio\",\n",
    "    \"mean_insertion_length_chars_exclu_space\",\n",
    "    \"total_deletions_words\",\n",
    "    \"mean_pause_time_before_sents_threshold200\",\n",
    "    \"mean_deletion_length_chars\",\n",
    "    \"num_of_insertions\",\n",
    "    \"median_insertion_length_chars_exclu_space\",\n",
    "    \"num_of_pause_within_words_threshold200\",\n",
    "    \"sd_strokes_per_min_5_intervals\",\n",
    "    \"median_length_Rburst_sec\",\n",
    "    \"median_pause_time_between_words_threshold200\",\n",
    "    \"num_of_pause_after_words_threshold200\",\n",
    "    \"num_of_revisions\",\n",
    "    \"sd_pause_time_before_words_threshold200\",\n",
    "    \"total_number_of_pauses_threshold2000\",\n",
    "    \"median_pause_time_before_words_threshold200\",\n",
    "    \"mean_pause_time_before_words_threshold200\",\n",
    "]\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ================================================================\n",
    "# 1. Load enriched JSONL   (train‑* vs valid‑*/test‑*)\n",
    "# ================================================================\n",
    "def load_dataset(folder: str, label: int, split: str):\n",
    "    \"\"\"split='train'  -> train-* files\n",
    "       split='eval'   -> valid-* and test-* files\"\"\"\n",
    "    rows = []\n",
    "    for fn in os.listdir(folder):\n",
    "        if not fn.endswith(\"_enriched.jsonl\"):       # skip raw files\n",
    "            continue\n",
    "        is_train = fn.startswith(\"train-\")\n",
    "        if (split == \"train\" and not is_train) or (split == \"eval\" and is_train):\n",
    "            continue\n",
    "        for line in open(Path(folder)/fn, encoding=\"utf-8\"):\n",
    "            obj  = json.loads(line)\n",
    "            meta = [obj[\"extra\"].get(c, 0.0) for c in META_COLS]\n",
    "            rows.append({\"text\": obj[\"text\"], \"meta\": meta, \"label\": label})\n",
    "    return rows\n",
    "\n",
    "train_rows = (load_dataset(\"OpenLLMText_Human\", 0, \"train\") +\n",
    "              load_dataset(\"OpenLLMText_ChatGPT\", 1, \"train\"))\n",
    "eval_rows  = (load_dataset(\"OpenLLMText_Human\", 0, \"eval\")  +\n",
    "              load_dataset(\"OpenLLMText_ChatGPT\", 1, \"eval\"))\n",
    "\n",
    "train_df = pd.DataFrame(train_rows)\n",
    "test_df  = pd.DataFrame(eval_rows)\n",
    "print(f\"Train DF: {len(train_df)}   –   Eval DF: {len(test_df)}\")\n",
    "\n",
    "# ================================================================\n",
    "# 2. Feature scaling  (fit on TRAIN only)\n",
    "# ================================================================\n",
    "scaler = StandardScaler().fit(np.vstack(train_df[\"meta\"]))\n",
    "train_df[\"meta\"] = list(scaler.transform(np.vstack(train_df[\"meta\"])))\n",
    "test_df [\"meta\"] = list(scaler.transform(np.vstack(test_df [\"meta\"])))\n",
    "\n",
    "# ================================================================\n",
    "# 3. PyTorch Dataset / Loader\n",
    "# ================================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(BACKBONE)\n",
    "\n",
    "class TextMetaDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts  = df[\"text\"].tolist()\n",
    "        self.metas  = torch.tensor(np.vstack(df[\"meta\"]), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(self.texts[idx], max_length=512, truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return {\"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "                \"meta\": self.metas[idx],\n",
    "                \"label\": self.labels[idx]}\n",
    "\n",
    "train_loader = DataLoader(TextMetaDS(train_df), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(TextMetaDS(test_df),  batch_size=BATCH_SIZE)\n",
    "\n",
    "# ================================================================\n",
    "# 4. Model definition\n",
    "# ================================================================\n",
    "class TextMetaClassifier(nn.Module):\n",
    "    def __init__(self, meta_dim=20, hidden_meta=32):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(BACKBONE)\n",
    "        # freeze embeddings + first 6 layers\n",
    "        for p in self.encoder.embeddings.parameters(): p.requires_grad = False\n",
    "        for layer in self.encoder.encoder.layer[:6]:\n",
    "            for p in layer.parameters(): p.requires_grad = False\n",
    "        self.meta_fc = nn.Sequential(nn.Linear(meta_dim, hidden_meta),\n",
    "                                     nn.ReLU(), nn.Dropout(0.1))\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size + hidden_meta, 1)\n",
    "    def forward(self, ids, mask, meta):\n",
    "        h_text = self.encoder(ids, attention_mask=mask).pooler_output\n",
    "        h_meta = self.meta_fc(meta)\n",
    "        return self.classifier(torch.cat([h_text, h_meta], dim=1)).squeeze(1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = TextMetaClassifier().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optim     = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                              lr=LR)\n",
    "\n",
    "# ================================================================\n",
    "# 5. Train & evaluate\n",
    "# ================================================================\n",
    "def run(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    losses, preds, trues = [], [], []\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for batch in tqdm(loader, leave=False):\n",
    "            ids  = batch[\"input_ids\"].to(device)\n",
    "            msk  = batch[\"attention_mask\"].to(device)\n",
    "            meta = batch[\"meta\"].to(device)\n",
    "            y    = batch[\"label\"].to(device)\n",
    "            logits = model(ids, msk, meta)\n",
    "            loss   = criterion(logits, y)\n",
    "            if train:\n",
    "                optim.zero_grad(); loss.backward(); optim.step()\n",
    "            losses.append(loss.item())\n",
    "            preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "            trues.extend(y.cpu().numpy())\n",
    "    return np.array(preds), np.array(trues), np.mean(losses)\n",
    "\n",
    "for ep in range(2, EPOCHS+1):\n",
    "    _, _, tr_loss = run(train_loader, True)\n",
    "    y_pred, y_true, val_loss = run(test_loader, False)\n",
    "    f1  = f1_score(y_true, y_pred >= 0.5)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred >= 0.5).ravel()\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    print(f\"[Ep {ep}] train_loss={tr_loss:.4f}  val_loss={val_loss:.4f}  \"\n",
    "          f\"F1={f1:.3f}  ROC‑AUC={auc:.3f}  FP‑rate={fp_rate:.2%}\")\n",
    "\n",
    "# ================================================================\n",
    "# 6. Save artefacts\n",
    "# ================================================================\n",
    "Path(\"saved\").mkdir(exist_ok=True)\n",
    "torch.save(model.state_dict(), \"saved/text_meta_split.pt\")\n",
    "joblib.dump(scaler, \"saved/meta_scaler_split.gz\")\n",
    "print(\"✔️  Model + scaler saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04fb2af3-db7d-40a9-8e6f-ed3512493dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size : 51874 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=0.3162 val_loss=0.2698  F1=0.936  ROC‑AUC=0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=0.2156 val_loss=0.1761  F1=0.977  ROC‑AUC=0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 192\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(losses), np.array(preds), np.array(trues)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     tr_loss, _, _       = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     val_loss, y_hat, y_true = run_epoch(test_loader,  train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    195\u001b[39m     f1  = f1_score(y_true, y_hat >= \u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(loader, train)\u001b[39m\n\u001b[32m    183\u001b[39m     loss.backward()\n\u001b[32m    184\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    187\u001b[39m preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n\u001b[32m    188\u001b[39m trues.extend(y.cpu().numpy())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Train Text+Meta Classifier  (Human vs ChatGPT) – English‑only version\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Prérequis :\n",
    "    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    pip install transformers scikit-learn pandas tqdm joblib\n",
    "\n",
    "Données attendues :\n",
    "    OpenLLMText_Human/*_enriched.jsonl\n",
    "    OpenLLMText_ChatGPT/*_enriched.jsonl\n",
    "\n",
    "Sorties :\n",
    "    saved/text_meta_roberta.pt\n",
    "    saved/meta_scaler.gz\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, os, random, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "# ────────────────────────────\n",
    "# 1. PARAMÈTRES\n",
    "# ────────────────────────────\n",
    "SEED       = 42\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 3\n",
    "LR         = 2e-5\n",
    "BACKBONE   = \"roberta-base\"     # full‑English\n",
    "\n",
    "META_COLS = [\n",
    "    \"mean_pause_time_in_secs_threshold200\",\n",
    "    \"total_insertions_chars_exclu_space\",\n",
    "    \"total_pause_time_in_secs_threshold2000\",\n",
    "    \"product_process_ratio\",\n",
    "    \"mean_insertion_length_chars_exclu_space\",\n",
    "    \"total_deletions_words\",\n",
    "    \"mean_pause_time_before_sents_threshold200\",\n",
    "    \"mean_deletion_length_chars\",\n",
    "    \"num_of_insertions\",\n",
    "    \"median_insertion_length_chars_exclu_space\",\n",
    "    \"num_of_pause_within_words_threshold200\",\n",
    "    \"sd_strokes_per_min_5_intervals\",\n",
    "    \"median_length_Rburst_sec\",\n",
    "    \"median_pause_time_between_words_threshold200\",\n",
    "    \"num_of_pause_after_words_threshold200\",\n",
    "    \"num_of_revisions\",\n",
    "    \"sd_pause_time_before_words_threshold200\",\n",
    "    \"total_number_of_pauses_threshold2000\",\n",
    "    \"median_pause_time_before_words_threshold200\",\n",
    "    \"mean_pause_time_before_words_threshold200\",\n",
    "]\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ────────────────────────────\n",
    "# 2. LECTURE DES JSONL ENRICHIS\n",
    "# ────────────────────────────\n",
    "def load_dataset(folder: str, label: int):\n",
    "    data = []\n",
    "    for fname in os.listdir(folder):\n",
    "        if fname.endswith(\"_enriched.jsonl\"):\n",
    "            for line in open(Path(folder) / fname, encoding=\"utf-8\"):\n",
    "                obj = json.loads(line)\n",
    "                meta = [obj[\"extra\"].get(col, 0.0) for col in META_COLS]\n",
    "                data.append({\"text\": obj[\"text\"], \"meta\": meta, \"label\": label})\n",
    "    return data\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    load_dataset(\"OpenLLMText_Human\", 0) +\n",
    "    load_dataset(\"OpenLLMText_ChatGPT\", 1)\n",
    ")\n",
    "print(f\"Dataset size : {len(data)} examples\")\n",
    "\n",
    "# ────────────────────────────\n",
    "# 3. TRAIN / TEST SPLIT\n",
    "# ────────────────────────────\n",
    "train_df, test_df = train_test_split(\n",
    "    data, test_size=0.2, random_state=SEED, stratify=data[\"label\"]\n",
    ")\n",
    "\n",
    "# Standardisation des 20 features\n",
    "scaler = StandardScaler().fit(np.vstack(train_df[\"meta\"]))\n",
    "train_df[\"meta\"] = list(scaler.transform(np.vstack(train_df[\"meta\"])))\n",
    "test_df[\"meta\"]  = list(scaler.transform(np.vstack(test_df[\"meta\"])))\n",
    "\n",
    "# ────────────────────────────\n",
    "# 4. DATASET PyTorch\n",
    "# ────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(BACKBONE)\n",
    "\n",
    "class TextMetaDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts  = df[\"text\"].tolist()\n",
    "        self.metas  = torch.tensor(np.vstack(df[\"meta\"]), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"meta\": self.metas[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_loader = DataLoader(TextMetaDS(train_df), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(TextMetaDS(test_df),  batch_size=BATCH_SIZE)\n",
    "\n",
    "# ────────────────────────────\n",
    "# 5. MODÈLE\n",
    "# ────────────────────────────\n",
    "class TextMetaClassifier(nn.Module):\n",
    "    def __init__(self, meta_dim=20, hidden_meta=32):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(BACKBONE)\n",
    "        # Freeze embeddings + 6 premières couches\n",
    "        for p in self.encoder.embeddings.parameters():\n",
    "            p.requires_grad = False\n",
    "        for layer in self.encoder.encoder.layer[:6]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.meta_fc = nn.Sequential(\n",
    "            nn.Linear(meta_dim, hidden_meta),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        self.classifier = nn.Linear(\n",
    "            self.encoder.config.hidden_size + hidden_meta, 1\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, meta):\n",
    "        h_text = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).pooler_output               # (bs, 768)\n",
    "        h_meta = self.meta_fc(meta)    # (bs, hidden_meta)\n",
    "        h = torch.cat([h_text, h_meta], dim=1)\n",
    "        return self.classifier(h).squeeze(1)  # logits\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = TextMetaClassifier().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "\n",
    "# ────────────────────────────\n",
    "# 6. BOUCLE D’ENTRAÎNEMENT\n",
    "# ────────────────────────────\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    losses, preds, trues = [], [], []\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for batch in tqdm(loader, leave=False):\n",
    "            ids  = batch[\"input_ids\"].to(device)\n",
    "            msk  = batch[\"attention_mask\"].to(device)\n",
    "            meta = batch[\"meta\"].to(device)\n",
    "            y    = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(ids, msk, meta)\n",
    "            loss   = criterion(logits, y)\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "            trues.extend(y.cpu().numpy())\n",
    "    return np.mean(losses), np.array(preds), np.array(trues)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, _, _       = run_epoch(train_loader, train=True)\n",
    "    val_loss, y_hat, y_true = run_epoch(test_loader,  train=False)\n",
    "\n",
    "    f1  = f1_score(y_true, y_hat >= 0.5)\n",
    "    auc = roc_auc_score(y_true, y_hat)\n",
    "    print(f\"[Epoch {epoch}] train_loss={tr_loss:.4f} \"\n",
    "          f\"val_loss={val_loss:.4f}  F1={f1:.3f}  ROC‑AUC={auc:.3f}\")\n",
    "\n",
    "# ────────────────────────────\n",
    "# 7. SAUVEGARDE\n",
    "# ────────────────────────────\n",
    "Path(\"saved\").mkdir(exist_ok=True)\n",
    "torch.save(model.state_dict(), \"saved/text_meta_roberta.pt\")\n",
    "joblib.dump(scaler,              \"saved/meta_scaler.gz\")\n",
    "print(\"🎉  Modèle et scaler sauvegardés dans ./saved/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f044f18-2762-4f06-9724-7587500ca400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉  Modèle et scaler sauvegardés dans ./saved/\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de6e333a-c98e-45d6-a266-0a3c1ce42c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 51874 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=0.3437 val_loss=0.3292  F1=0.827  ROC‑AUC=0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=0.3199 val_loss=0.3222  F1=0.848  ROC‑AUC=0.866\n",
      "✅  Text‑only model saved to ./saved/\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Text‑only LLM classifier  — Human vs ChatGPT (English)\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Entrée  : OpenLLMText_Human/*_enriched.jsonl\n",
    "          OpenLLMText_ChatGPT/*_enriched.jsonl\n",
    "Sorties : saved/text_only_roberta.pt\n",
    "          (aucun scaler, car pas de variables méta)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, os, random, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─── PARAMS ──────────────────────────────────────────────────────────\n",
    "SEED       = 42\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 2\n",
    "LR         = 2e-5\n",
    "BACKBONE   = \"roberta-base\"\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ─── 1. DATA LOAD (ignore les méta) ──────────────────────────────────\n",
    "def load_dataset(folder: str, label: int):\n",
    "    data = []\n",
    "    for fn in os.listdir(folder):\n",
    "        if fn.endswith(\"_enriched.jsonl\"):\n",
    "            for line in open(Path(folder)/fn, encoding=\"utf-8\"):\n",
    "                obj = json.loads(line)\n",
    "                data.append({\"text\": obj[\"text\"], \"label\": label})\n",
    "    return data\n",
    "\n",
    "df = pd.DataFrame(\n",
    "      load_dataset(\"OpenLLMText_Human\", 0)\n",
    "    + load_dataset(\"OpenLLMText_ChatGPT\", 1)\n",
    ")\n",
    "print(f\"Dataset size: {len(df)} examples\")\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# ─── 2. DATASET / DATALOADER ─────────────────────────────────────────\n",
    "tok = AutoTokenizer.from_pretrained(BACKBONE)\n",
    "\n",
    "class TextOnlyDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts  = df[\"text\"].tolist()\n",
    "        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tok(self.texts[idx], max_length=512, truncation=True,\n",
    "                  padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return {\"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "                \"label\": self.labels[idx]}\n",
    "\n",
    "train_loader = DataLoader(TextOnlyDS(train_df), BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(TextOnlyDS(test_df),  BATCH_SIZE)\n",
    "\n",
    "# ─── 3. MODEL ────────────────────────────────────────────────────────\n",
    "class TextOnlyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = AutoModel.from_pretrained(BACKBONE)\n",
    "        # option : geler embeddings + moitié des couches\n",
    "        for p in self.enc.embeddings.parameters():\n",
    "            p.requires_grad = False\n",
    "        for layer in self.enc.encoder.layer[:6]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.clf = nn.Linear(self.enc.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        h = self.enc(ids, attention_mask=mask).pooler_output\n",
    "        return self.clf(h).squeeze(1)     # logits\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = TextOnlyClassifier().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optim     = torch.optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "                                     model.parameters()), lr=LR)\n",
    "\n",
    "# ─── 4. TRAIN / EVAL LOOP ────────────────────────────────────────────\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    losses, preds, trues = [], [], []\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for batch in tqdm(loader, leave=False):\n",
    "            ids  = batch[\"input_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            y    = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(ids, mask)\n",
    "            loss   = criterion(logits, y)\n",
    "            if train:\n",
    "                optim.zero_grad(); loss.backward(); optim.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "            trues.extend(y.cpu().numpy())\n",
    "    return np.mean(losses), np.array(preds), np.array(trues)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss, _, _        = run_epoch(train_loader, True)\n",
    "    val_loss, y_hat, yt  = run_epoch(test_loader,  False)\n",
    "    f1  = f1_score(yt, y_hat >= 0.5)\n",
    "    auc = roc_auc_score(yt, y_hat)\n",
    "    print(f\"[Epoch {ep}] train_loss={tr_loss:.4f} \"\n",
    "          f\"val_loss={val_loss:.4f}  F1={f1:.3f}  ROC‑AUC={auc:.3f}\")\n",
    "\n",
    "# ─── 5. SAVE MODEL ───────────────────────────────────────────────────\n",
    "Path(\"saved\").mkdir(exist_ok=True)\n",
    "torch.save(model.state_dict(), \"saved/text_only_roberta.pt\")\n",
    "print(\"✅  Text‑only model saved to ./saved/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "240b319b-0e12-488a-90ca-030aa447b415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_965/3509756482.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"saved/text_meta_roberta.pt\",\n",
      "100%|██████████| 649/649 [04:52<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1=0.969  ROC‑AUC=0.986  FP‑rate=19.460%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate_f1.py --------------------------------------------------------\n",
    "import json, os, joblib, torch, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ----- 1. Reprend la même classe que dans ton training -----------------\n",
    "class TextMetaClassifier(torch.nn.Module):\n",
    "    def __init__(self, meta_dim=20, hidden_meta=32, backbone=\"roberta-base\"):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(backbone)\n",
    "        for p in self.encoder.embeddings.parameters():\n",
    "            p.requires_grad = False\n",
    "        for layer in self.encoder.encoder.layer[:6]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.meta_fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(meta_dim, hidden_meta),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "        )\n",
    "        self.classifier = torch.nn.Linear(\n",
    "            self.encoder.config.hidden_size + hidden_meta, 1\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, meta):\n",
    "        h_text = self.encoder(input_ids, attention_mask).pooler_output\n",
    "        h_meta = self.meta_fc(meta)\n",
    "        h = torch.cat([h_text, h_meta], dim=1)\n",
    "        return self.classifier(h).squeeze(1)  # logits\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# ----- 2. paramètres ---------------------------------------------------\n",
    "SEED, BATCH = 42, 16\n",
    "BACKBONE = \"roberta-base\"\n",
    "META_COLS = [\n",
    "    \"mean_pause_time_in_secs_threshold200\",\n",
    "    \"total_insertions_chars_exclu_space\",\n",
    "    \"total_pause_time_in_secs_threshold2000\",\n",
    "    \"product_process_ratio\",\n",
    "    \"mean_insertion_length_chars_exclu_space\",\n",
    "    \"total_deletions_words\",\n",
    "    \"mean_pause_time_before_sents_threshold200\",\n",
    "    \"mean_deletion_length_chars\",\n",
    "    \"num_of_insertions\",\n",
    "    \"median_insertion_length_chars_exclu_space\",\n",
    "    \"num_of_pause_within_words_threshold200\",\n",
    "    \"sd_strokes_per_min_5_intervals\",\n",
    "    \"median_length_Rburst_sec\",\n",
    "    \"median_pause_time_between_words_threshold200\",\n",
    "    \"num_of_pause_after_words_threshold200\",\n",
    "    \"num_of_revisions\",\n",
    "    \"sd_pause_time_before_words_threshold200\",\n",
    "    \"total_number_of_pauses_threshold2000\",\n",
    "    \"median_pause_time_before_words_threshold200\",\n",
    "    \"mean_pause_time_before_words_threshold200\",\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(BACKBONE)\n",
    "\n",
    "# ----- 3. Recharger le jeu de données enrichi --------------------------\n",
    "def load_dataset(folder, label):\n",
    "    rows = []\n",
    "    for fn in os.listdir(folder):\n",
    "        if fn.endswith(\"_enriched.jsonl\"):\n",
    "            for line in open(Path(folder)/fn, encoding=\"utf-8\"):\n",
    "                obj = json.loads(line)\n",
    "                meta = [obj[\"extra\"].get(c, 0.0) for c in META_COLS]\n",
    "                rows.append({\"text\": obj[\"text\"], \"meta\": meta, \"label\": label})\n",
    "    return rows\n",
    "\n",
    "df = pd.DataFrame(\n",
    "      load_dataset(\"OpenLLMText_Human\", 0)\n",
    "    + load_dataset(\"OpenLLMText_ChatGPT\", 1)\n",
    ")\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# ----- 4. Appliquer le scaler sauvegardé -------------------------------\n",
    "scaler = joblib.load(\"saved/meta_scaler.gz\")\n",
    "test_df[\"meta\"] = list(scaler.transform(np.vstack(test_df[\"meta\"])))\n",
    "\n",
    "# ----- 5. DataLoader ----------------------------------------------------\n",
    "class TextMetaDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts  = df[\"text\"].tolist()\n",
    "        self.metas  = torch.tensor(np.vstack(df[\"meta\"]), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = tokenizer(self.texts[i], max_length=512, truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return {\"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "                \"meta\": self.metas[i],\n",
    "                \"label\": self.labels[i]}\n",
    "\n",
    "loader = DataLoader(TextMetaDS(test_df), batch_size=BATCH)\n",
    "\n",
    "# ----- 6. Charger le modèle entraîné et évaluer ------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TextMetaClassifier(backbone=BACKBONE).to(device)\n",
    "model.load_state_dict(torch.load(\"saved/text_meta_roberta.pt\",\n",
    "                                 map_location=device))\n",
    "model.eval()\n",
    "\n",
    "pred, true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader):\n",
    "        ids  = batch[\"input_ids\"].to(device)\n",
    "        msk  = batch[\"attention_mask\"].to(device)\n",
    "        meta = batch[\"meta\"].to(device)\n",
    "        y    = batch[\"label\"].to(device)\n",
    "        logits = model(ids, msk, meta)\n",
    "        pred.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "        true.extend(y.cpu().numpy())\n",
    "\n",
    "pred, true = np.array(pred), np.array(true)\n",
    "f1  = f1_score(true, pred >= 0.5)\n",
    "auc = roc_auc_score(true, pred)\n",
    "tn, fp, fn, tp = confusion_matrix(true, pred >= 0.5).ravel()\n",
    "fp_rate = fp / (fp + tn)\n",
    "print(f\"F1={f1:.3f}  ROC‑AUC={auc:.3f}  FP‑rate={fp_rate:.3%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "578ed2b8-8ced-4e5c-9750-aa6e2e5607fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_965/2555200125.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"saved/text_only_roberta.pt\",\n",
      "100%|██████████| 649/649 [04:56<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1=0.848   ROC‑AUC=0.866   FP‑rate=22.289%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Evaluate text‑only RoBERTa model and compute FP‑rate\n",
    "import json, os, torch, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------- parameters ------------------------------------------\n",
    "SEED, BATCH = 42, 16\n",
    "BACKBONE = \"roberta-base\"\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---------------- load enriched JSONL (text only) ---------------------\n",
    "def load_dataset(folder, label):\n",
    "    rows = []\n",
    "    for fn in os.listdir(folder):\n",
    "        if fn.endswith(\"_enriched.jsonl\"):\n",
    "            for line in open(Path(folder)/fn, encoding=\"utf-8\"):\n",
    "                obj = json.loads(line)\n",
    "                rows.append({\"text\": obj[\"text\"], \"label\": label})\n",
    "    return rows\n",
    "\n",
    "df = pd.DataFrame(\n",
    "      load_dataset(\"OpenLLMText_Human\", 0)\n",
    "    + load_dataset(\"OpenLLMText_ChatGPT\", 1)\n",
    ")\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# ---------------- DataLoader ------------------------------------------\n",
    "tok = AutoTokenizer.from_pretrained(BACKBONE)\n",
    "class TextOnlyDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts  = df[\"text\"].tolist()\n",
    "        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = tok(self.texts[i], max_length=512, truncation=True,\n",
    "                  padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return {\"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "                \"label\": self.labels[i]}\n",
    "\n",
    "loader = DataLoader(TextOnlyDS(test_df), batch_size=BATCH)\n",
    "\n",
    "# ---------------- model definition (same as training) -----------------\n",
    "class TextOnlyClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = AutoModel.from_pretrained(BACKBONE)\n",
    "        for p in self.enc.embeddings.parameters():\n",
    "            p.requires_grad = False\n",
    "        for layer in self.enc.encoder.layer[:6]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.clf = torch.nn.Linear(self.enc.config.hidden_size, 1)\n",
    "    def forward(self, ids, mask):\n",
    "        h = self.enc(ids, attention_mask=mask).pooler_output\n",
    "        return self.clf(h).squeeze(1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = TextOnlyClassifier().to(device)\n",
    "model.load_state_dict(torch.load(\"saved/text_only_roberta.pt\",\n",
    "                                 map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ---------------- inference & metrics ---------------------------------\n",
    "pred, true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader):\n",
    "        ids  = batch[\"input_ids\"].to(device)\n",
    "        msk  = batch[\"attention_mask\"].to(device)\n",
    "        y    = batch[\"label\"].to(device)\n",
    "        logits = model(ids, msk)\n",
    "        pred.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "        true.extend(y.cpu().numpy())\n",
    "\n",
    "pred, true = np.array(pred), np.array(true)\n",
    "threshold = 0.5\n",
    "f1  = f1_score(true, pred >= threshold)\n",
    "auc = roc_auc_score(true, pred)\n",
    "tn, fp, fn, tp = confusion_matrix(true, pred >= threshold).ravel()\n",
    "fp_rate = fp / (fp + tn)\n",
    "\n",
    "print(f\"F1={f1:.3f}   ROC‑AUC={auc:.3f}   FP‑rate={fp_rate:.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694243fb-fc6a-4d59-a734-32d5bef12cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule à lancer quand train_df / test_df existent déjà\n",
    "#  et contiennent : text, meta (list[20]), label\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import joblib, os\n",
    "from pathlib import Path\n",
    "\n",
    "SEED, BATCH, EPOCHS, LR = 42, 64, 3, 3e-4\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# 1 ── NORMALISER les 20 features ---------------------------------------\n",
    "scaler = StandardScaler().fit(np.vstack(train_df[\"meta\"]))\n",
    "train_df[\"meta\"] = list(scaler.transform(np.vstack(train_df[\"meta\"])))\n",
    "test_df [\"meta\"] = list(scaler.transform(np.vstack(test_df [\"meta\"])))\n",
    "\n",
    "# 2 ── DATASET / LOADER --------------------------------------------------\n",
    "class MetaDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.x = torch.tensor(np.vstack(df[\"meta\"]), dtype=torch.float32)\n",
    "        self.y = torch.tensor(df[\"label\"].values,     dtype=torch.float32)\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return {\"meta\": self.x[i], \"label\": self.y[i]}\n",
    "\n",
    "train_loader = DataLoader(MetaDS(train_df), BATCH, shuffle=True)\n",
    "test_loader  = DataLoader(MetaDS(test_df),  BATCH)\n",
    "\n",
    "# 3 ── MODELE ------------------------------------------------------------\n",
    "class MetaOnlyMLP(nn.Module):\n",
    "    def __init__(self, meta_dim=20, hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(meta_dim, hidden), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(hidden, hidden//2), nn.ReLU(),\n",
    "            nn.Linear(hidden//2, 1)\n",
    "        )\n",
    "    def forward(self, m): return self.net(m).squeeze(1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = MetaOnlyMLP().to(device)\n",
    "lossf  = nn.BCEWithLogitsLoss()\n",
    "optim  = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# 4 ── BOUCLE ------------------------------------------------------------\n",
    "def run(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    preds, trues, losses = [], [], []\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for batch in tqdm(loader, leave=False):\n",
    "            m = batch[\"meta\"].to(device); y = batch[\"label\"].to(device)\n",
    "\n",
    "            logit = model(m)\n",
    "            loss  = lossf(logit, y)\n",
    "\n",
    "            if train:\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds.extend(torch.sigmoid(logit).detach().cpu().numpy())  # ← fix\n",
    "            trues.extend(y.cpu().numpy())\n",
    "    return np.array(preds), np.array(trues), np.mean(losses)\n",
    "\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    _, _, tr_loss       = run(train_loader, True)\n",
    "    y_pred, y_true, vl  = run(test_loader,  False)\n",
    "    f1  = f1_score(y_true, y_pred >= 0.5)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    tn, fp, _, _ = confusion_matrix(y_true, y_pred >= 0.5).ravel()\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    print(f\"[Ep {ep}] train_loss={tr_loss:.4f}  val_loss={vl:.4f}  \"\n",
    "          f\"F1={f1:.3f}  ROC‑AUC={auc:.3f}  FP‑rate={fp_rate:.2%}\")\n",
    "\n",
    "# 5 ── SAUVEGARDE --------------------------------------------------------\n",
    "Path(\"saved\").mkdir(exist_ok=True)\n",
    "torch.save(model.state_dict(), \"saved/meta_only_mlp_split.pt\")\n",
    "joblib.dump(scaler, \"saved/meta_scaler_split.gz\")\n",
    "print(\"✔️  Meta‑only model + scaler saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
